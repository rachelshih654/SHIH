{
  "hash": "4518d65552cbf8d247ed8b71a5424847",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"迴歸分析實例\"\ndescription: \"\"\ndate: \"2025-07-31\"\njupyter: python3\nexecute:\n    echo: false  # 是否顯示代碼\nformat:\n  html:\n    code-fold: true\n    code-summary: \"顯示／隱藏程式碼\"\n    code-tools: true\n---\n\n\n\n## 範例一：線性迴歸概念及scikit-learn 實作\n\n以動物體重與奔跑速度作為範例，使用 比較公式計算與scikit-learn 實作的結果\n\n::: {.columns}\n\n::: {.column style=\"width:45%; padding-right: 2rem;\"}\n\n### 資料\n\n::: {#59d4ba58 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=2}\n| 動物   |   動物體重 (kg) |   最大奔跑速度 (km/h) |\n|:-------|----------------:|----------------------:|\n| 犀牛   |            1400 |                    45 |\n| 馬     |             400 |                    70 |\n| 羚羊   |              50 |                   100 |\n| 長頸鹿 |            1000 |                    60 |\n| 斑馬   |             300 |                    90 |\n| 獵豹   |              60 |                   110 |\n:::\n:::\n\n\n:::\n\n::: {.column style=\"width:55%; margin-left: auto; margin-right: 0;\"}\n\n### 散點圖\n\n::: {#9e056310 .cell fig-format='svg' fig-height='3' fig-width='4' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=414 height=310 fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n### 公式計算\n\n$$\n\\begin{align}\n\\hat{{\\mathbf{\\beta}}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\end{align}\n$$\n\n::: {#32ec0656 .cell execution_count=4}\n``` {.python .cell-code}\n# 依照公式 \\hat{beta} = (X^T X)^{-1} X^T y 計算參數\nX_raw = df_lm[\"動物體重 (kg)\"].to_numpy(dtype=float)\ny_vec = df_lm[\"最大奔跑速度 (km/h)\"].to_numpy(dtype=float)\n\n# 設計矩陣：第一欄為截距(常數1)，第二欄為特徵 X\nX_design = np.column_stack([np.ones(len(X_raw)), X_raw])\n\n# 依公式計算參數向量 [a, b]\nbeta_hat = np.linalg.inv(X_design.T @ X_design) @ (X_design.T @ y_vec)\na, b = float(beta_hat[0]), float(beta_hat[1])\n\nprint(f\"公式計算得出的迴歸模型: y={b:.3f}x+{a:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n公式計算得出的迴歸模型: y=-0.043x+102.200\n```\n:::\n:::\n\n\n### scikit-learn 建立迴歸模型\n\n::: {#432ca171 .cell execution_count=5}\n``` {.python .cell-code}\n# 建立迴歸模型\nfrom sklearn.linear_model import LinearRegression\n\n# 特徵與目標（注意特徵需為 2D）\nfeatures_X = df_lm[[\"動物體重 (kg)\"]].values\ntarget_y = df_lm[\"最大奔跑速度 (km/h)\"].values\n\nmodel = LinearRegression()\n\n# 訓練模型\nmodel.fit(features_X, target_y)\n\nprint(f\"scikit-learn 計算得出的迴歸模型: y={model.coef_[0].round(3)}x+{model.intercept_.round(3)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nscikit-learn 計算得出的迴歸模型: y=-0.043x+102.2\n```\n:::\n:::\n\n\n::: {#e9763fb4 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=599 height=458}\n:::\n:::\n\n\n### 預測\n\n::: {#f7bd9b81 .cell execution_count=7}\n``` {.python .cell-code}\n# 預測\nx = 1000\nsklearn_y = model.predict([[x]])\nformula_y = a + b * x\n\nprint(\n    f\"預測 {x}kg 的動物最大奔跑速度，\\n\"\n    f\"scikit-learn 預測值為 {sklearn_y[0].round(3)} km/h，\\n\"\n    f\"公式計算得出的預測值為 {formula_y:.3f} km/h\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n預測 1000kg 的動物最大奔跑速度，\nscikit-learn 預測值為 59.147 km/h，\n公式計算得出的預測值為 59.147 km/h\n```\n:::\n:::\n\n\n可知，scikit-learn 與公式計算得出的預測值相同，原因是 scikit-learn 在背後也是使用公式計算的。\n\n## 範例二：邏輯迴歸與線性迴歸的比較\n\n假設我們想預測學生是否會及格，資料如下，X軸代表每日學習時數，Y軸代表是否及格\n\n::: {.columns}\n\n::: {.column style=\"width:45%; padding-right: 2rem;\"}\n\n\n### 資料\n\n::: {#61f5393b .cell execution_count=8}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=8}\n| 學生   |   學習時數 |   是否及格 |\n|:-------|-----------:|-----------:|\n| A      |          1 |          0 |\n| B      |          2 |          1 |\n| C      |          3 |          0 |\n| D      |          5 |          0 |\n| E      |          5 |          1 |\n| F      |          8 |          1 |\n| G      |          8 |          1 |\n| H      |          8 |          0 |\n| I      |          9 |          1 |\n| J      |         10 |          1 |\n:::\n:::\n\n\n:::\n\n::: {.column style=\"width:55%; margin-left: auto; margin-right: 0;\"}\n\n### 散點圖\n\n::: {#608d9cf8 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=367 height=310}\n:::\n:::\n\n\n:::\n\n:::\n\n上方的資料可以明顯看出，Y軸的值屬於二元變數，是離散型的。從散點圖中可以看出，學習時數與是否及格之間的關係並非線性。\n\n使用 scikit-learn 取得模型參數，並畫出 sigmoid 曲線，紅色點為原始資料，藍色點為經過 sigmoid 函數轉換後的值，藍色線為根據原始資料所作的線性迴歸，紅色線為 sigmoid 曲線。\n\n::: {#9c850c45 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\nX = logistic_df[[\"學習時數\"]].values  # 特徵（輸入）\nY = logistic_df[\"是否及格\"].values  # 標籤（目標）\n\n\n# 2. 建立並訓練邏輯回歸模型\nmodel = LogisticRegression()\nmodel.fit(X, Y)\n\n# 3. 取得模型參數（截距和斜率）\nbeta_0 = model.intercept_[0]\nbeta_1 = model.coef_[0][0]\n\nprint(f\"σ(z)模型參數：beta_0 = {beta_0:.4f}, beta_1 = {beta_1:.4f}\")\n\n\n# 4. 計算每個 X 對應的 z 和 sigmoid(z)\nz = beta_0 + beta_1 * X.flatten()\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\npredicted_probabilities = sigmoid(z)\n\n# 顯示每筆資料的 z 與預測機率\nfor i in range(len(X)):\n    print(f\"X = {X[i][0]}, z = {z[i]:.4f}, σ(z) = {predicted_probabilities[i]:.4f}\")\n\n# 5. 畫 sigmoid 曲線\nx_min, x_max = X.min() - 3, X.max() + 3\nx_range = np.linspace(x_min, x_max, 300)\nz_range = beta_0 + beta_1 * x_range\nsigmoid_curve = sigmoid(z_range)\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(7, 4))\nplt.plot(x_range, sigmoid_curve, label=\"Sigmoid Curve\", linewidth=2)\n\n# 線性迴歸\nlin_model = LinearRegression()\nlin_model.fit(X, Y)\nlin_predictions = lin_model.predict(x_range.reshape(-1, 1))\nplt.plot(x_range, lin_predictions, label=\"線性迴歸\", linestyle=\"--\", linewidth=2)\n\n\n# 資料點\nplt.scatter(X, Y, color=\"red\", label=\"Original Data (Y)\", zorder=5)\nplt.scatter(\n    X,\n    predicted_probabilities,\n    color=\"blue\",\n    label=\"Predicted Probabilities\",\n    marker=\"x\",\n    zorder=5,\n)\nplt.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\nplt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nplt.axhline(1, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nplt.xlabel(\"X（學習時數）\")\nplt.ylabel(\"Y=1 的機率\")\nplt.title(\"邏輯迴歸：Sigmoid 曲線與機率\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nσ(z)模型參數：beta_0 = -1.4243, beta_1 = 0.3225\nX = 1, z = -1.1017, σ(z) = 0.2494\nX = 2, z = -0.7792, σ(z) = 0.3145\nX = 3, z = -0.4567, σ(z) = 0.3878\nX = 5, z = 0.1884, σ(z) = 0.5470\nX = 5, z = 0.1884, σ(z) = 0.5470\nX = 8, z = 1.1560, σ(z) = 0.7606\nX = 8, z = 1.1560, σ(z) = 0.7606\nX = 8, z = 1.1560, σ(z) = 0.7606\nX = 9, z = 1.4785, σ(z) = 0.8143\nX = 10, z = 1.8010, σ(z) = 0.8583\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=658 height=376}\n:::\n:::\n\n\ns型曲線更能展現出此資料分布的特性，而線性迴歸則無法。並且邏輯迴歸的預測值永遠介於 0~1，代表「Y=1 的機率」，而線性迴歸的預測值可以小於 0 或大於 1，沒有機率意義。\n\n### 預測\n\n::: {#aa9d9aa3 .cell execution_count=11}\n``` {.python .cell-code}\n# 預測\nx_new = np.array([[0], [3],[4.5], [6], [15]])\ny_logistic = model.predict_proba(x_new)[:, 1]\ny_linear = lin_model.predict(x_new)\n\n# 建立 DataFrame 表格\ndf_pred = pd.DataFrame(\n    {\n        \"學習時數\": x_new.flatten(),\n        \"邏輯回歸預測機率 (P=1)\": y_logistic.round(3),\n        \"線性回歸預測值\": y_linear.round(3),\n    }\n)\n\n# 顯示表格\nMarkdown(df_pred.to_markdown(index=False))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=11}\n|   學習時數 |   邏輯回歸預測機率 (P=1) |   線性回歸預測值 |\n|-----------:|-------------------------:|-----------------:|\n|        0   |                    0.194 |            0.162 |\n|        3   |                    0.388 |            0.385 |\n|        4.5 |                    0.507 |            0.496 |\n|        6   |                    0.625 |            0.607 |\n|       15   |                    0.968 |            1.276 |\n:::\n:::\n\n\n- **邏輯回歸預測值**：永遠介於 0~1，代表「Y=1 的機率」\n- **線性回歸預測值**：可以小於 0 或大於 1，沒有機率意義\n\n## 範例三：kaggle 50 家新創企業數據，線性迴歸的實際應用\n\n資料來源：https://www.kaggle.com/datasets/amineoumous/50-startups-data\n\n資料內容：50 家新創企業的資料，包括 「研發支出」、「管理」、「行銷支出」、「所在州」、「利潤」。前 3 列表示每家新創公司在研發、行銷和管理方面的支出；所在州列表示新創公司位於哪個州；最後一列表示新創公司的利潤。\n\n### 資料概述\n\n::: {#3d58ab2d .cell execution_count=12}\n\n::: {.cell-output .cell-output-stdout}\n```\n筆數：50，欄位數：5\nR&D Spend          float64\nAdministration     float64\nMarketing Spend    float64\nState               object\nProfit             float64\ndtype: object\n           R&D Spend  Administration  Marketing Spend         Profit\ncount      50.000000       50.000000        50.000000      50.000000\nmean    73721.615600   121344.639600    211025.097800  112012.639200\nstd     45902.256482    28017.802755    122290.310726   40306.180338\nmin         0.000000    51283.140000         0.000000   14681.400000\n25%     39936.370000   103730.875000    129300.132500   90138.902500\n50%     73051.080000   122699.795000    212716.240000  107978.190000\n75%    101602.800000   144842.180000    299469.085000  139765.977500\nmax    165349.200000   182645.560000    471784.100000  192261.830000\nState 的類別資料統計\nState\nNew York      17\nCalifornia    17\nFlorida       16\nName: count, dtype: int64\n|   R&D Spend |   Administration |   Marketing Spend | State      |   Profit |\n|------------:|-----------------:|------------------:|:-----------|---------:|\n|      165349 |         136898   |            471784 | New York   |   192262 |\n|      162598 |         151378   |            443899 | California |   191792 |\n|      153442 |         101146   |            407935 | Florida    |   191050 |\n|      144372 |         118672   |            383200 | New York   |   182902 |\n|      142107 |          91391.8 |            366168 | Florida    |   166188 |\n```\n:::\n:::\n\n\n### 探索分析(Exploratory Data Analysis, EDA)\n\n::: {#00b2cf55 .cell execution_count=13}\n``` {.python .cell-code}\n# formatter: #,##0\nfmt = mtick.StrMethodFormatter(\"{x:,.0f}\")\n\n# Profit 分布\nplt.figure(figsize=(7, 4))\nax = sns.histplot(startups_df[\"Profit\"], bins=30, kde=True) \nax.set_title(\"Profit 分布\")\nax.xaxis.set_major_formatter(fmt)  # 設定 x 軸數字格式\nplt.show()\n\n# 各支出與 Profit 的關係（散點圖 + 相關係數熱力圖）\nfig, axes = plt.subplots(2, 2, figsize=(7, 4))\n\n# R&D Spend vs Profit\nsns.scatterplot(\n    ax=axes[0, 0],\n    x=\"R&D Spend\",\n    y=\"Profit\",\n    data=startups_df,\n    hue=\"State\",\n    legend=\"full\",   # 保留 legend\n)\naxes[0, 0].set_title(\"R&D Spend 與 Profit 的關係\")\naxes[0, 0].legend_.remove()   # 把第一張圖的 legend 移掉\n\n# Marketing Spend vs Profit\nsns.scatterplot(\n    ax=axes[0, 1],\n    x=\"Marketing Spend\",\n    y=\"Profit\",\n    data=startups_df,\n    hue=\"State\",\n    legend=False,\n)\naxes[0, 1].set_title(\"Marketing Spend 與 Profit 的關係\")\n\n# Administration vs Profit\nsns.scatterplot(\n    ax=axes[1, 0],\n    x=\"Administration\",\n    y=\"Profit\",\n    data=startups_df,\n    hue=\"State\",\n    legend=False,\n)\naxes[1, 0].set_title(\"Administration 與 Profit 的關係\")\n\n# 把右下角的空白子圖清空\naxes[1, 1].axis(\"off\")\n\n# 從第一個子圖拿 legend 資訊\nhandles, labels = axes[0, 0].get_legend_handles_labels()\naxes[1, 1].legend(handles, labels, loc=\"center\")  # 放在右下角空白處\naxes[1, 1].set_title(\"Legend\", fontsize=10)       # 可選，加個標題\n\n\n# 設定每個子圖的 X, Y 軸格式\nfor ax in axes.ravel():\n    ax.xaxis.set_major_formatter(fmt)\n    ax.yaxis.set_major_formatter(fmt)\n\nplt.tight_layout()\nplt.show()\n\n# 相關係數熱力圖\nsns.heatmap(\n    startups_df[[\"R&D Spend\", \"Marketing Spend\", \"Administration\", \"Profit\"]].corr(),\n    annot=True,\n    cmap=\"coolwarm\",\n    linewidths=0.5,\n    mask=np.triu(\n        np.ones_like(\n            startups_df[\n                [\"R&D Spend\", \"Marketing Spend\", \"Administration\", \"Profit\"]\n            ].corr(),\n            dtype=bool,\n        )\n    ),\n)\nplt.title(\"相關係數熱力圖\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=601 height=384}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=693 height=375}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-3.png){width=625 height=436}\n:::\n:::\n\n\n可以看出，R&D Spend 與 Profit 的關係最強，Administration 與 Profit 的關係最弱。\n\n### 模型分析\n\n#### 簡單線性迴歸模型\n\n三個變數與 Profit 的關係，分別建立三個模型，繪製迴歸線，並計算 $R^2$。\n\n::: {#b4b0b118 .cell execution_count=14}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\n\nplt.style.use(\"ggplot\")\n\n\n# 建立模型一 : R&D Spend 與 Profit 的迴歸模型\nmodel_1 = LinearRegression()\nmodel_1.fit(startups_df[[\"R&D Spend\"]], startups_df[\"Profit\"])\nprint(f\"R&D Spend 與 Profit 的迴歸模型: y={model_1.coef_[0]:.3f}x+{model_1.intercept_:.3f}\")\n\n# 建立模型二 : Marketing Spend 與 Profit 的迴歸模型\nmodel_2 = LinearRegression()\nmodel_2.fit(startups_df[[\"Marketing Spend\"]], startups_df[\"Profit\"])\nprint(f\"Marketing Spend 與 Profit 的迴歸模型: y={model_2.coef_[0]:.3f}x+{model_2.intercept_:.3f}\")\n\n\n# 建立模型三 : Administration 與 Profit 的迴歸模型\nmodel_3 = LinearRegression()\nmodel_3.fit(startups_df[[\"Administration\"]], startups_df[\"Profit\"])\nprint(f\"Administration 與 Profit 的迴歸模型: y={model_3.coef_[0]:.3f}x+{model_3.intercept_:.3f}\")\n\n# 三個模型的R²\nr2_1 = model_1.score(startups_df[[\"R&D Spend\"]], startups_df[\"Profit\"])\nr2_2 = model_2.score(startups_df[[\"Marketing Spend\"]], startups_df[\"Profit\"])\nr2_3 = model_3.score(startups_df[[\"Administration\"]], startups_df[\"Profit\"])\n\n# 整理成 DataFrame（方便控制格式）\nr2__simple_df = pd.DataFrame({\n    \"變數\": [\"R&D Spend\", \"Marketing Spend\", \"Administration\"],\n    \"R²\": [r2_1, r2_2, r2_3]\n})\n\n\n# 繪製三種模型的迴歸線\nfig, axes = plt.subplots(2, 2, figsize=(7, 4))\naxes[0, 0].scatter(startups_df[\"R&D Spend\"], startups_df[\"Profit\"], color=\"gray\")\naxes[0, 0].plot(startups_df[\"R&D Spend\"], model_1.predict(startups_df[[\"R&D Spend\"]]), color=\"red\")\naxes[0, 0].set_title(\"R&D Spend 與 Profit 的迴歸模型\")\n\naxes[0, 1].scatter(startups_df[\"Marketing Spend\"], startups_df[\"Profit\"], color=\"gray\")\naxes[0, 1].plot(startups_df[\"Marketing Spend\"], model_2.predict(startups_df[[\"Marketing Spend\"]]), color=\"red\")\naxes[0, 1].set_title(\"Marketing Spend 與 Profit 的迴歸模型\")\n\naxes[1, 0].scatter(startups_df[\"Administration\"], startups_df[\"Profit\"], color=\"gray\")\naxes[1, 0].plot(startups_df[\"Administration\"], model_3.predict(startups_df[[\"Administration\"]]), color=\"red\")\naxes[1, 0].set_title(\"Administration 與 Profit 的迴歸模型\")\n\n# R² 表格\naxes[1, 1].axis(\"off\")\ntable = axes[1, 1].table(\n    cellText=r2__simple_df.round(3).values,  # 保留三位小數\n    colLabels=r2__simple_df.columns,\n    cellLoc=\"center\",\n    loc=\"center\"\n)\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.2)  # 放大表格\n\n# 設定每個子圖的 X, Y 軸格式\nfor ax in axes.ravel():\n    ax.xaxis.set_major_formatter(fmt)\n    ax.yaxis.set_major_formatter(fmt) \n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR&D Spend 與 Profit 的迴歸模型: y=0.854x+49032.899\nMarketing Spend 與 Profit 的迴歸模型: y=0.246x+60003.549\nAdministration 與 Profit 的迴歸模型: y=0.289x+76974.471\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){width=683 height=375}\n:::\n:::\n\n\n$R^2$ 值越接近 1，表示模型越能解釋變數的變化。\n\n如相關係數結果，R&D Spend 與 Profit 的相關係數為 0.975，表示兩者之間有很強的正相關。\n\n如迴歸模型結果，R&D Spend 與 Profit 的 R² 值為 0.93，表示模型能解釋 93% 的 Profit 變化。\n\n因此，R&D Spend 是影響 Profit 最重要的變數。\n\n#### 多元線性迴歸模型\n1. 考慮 R&D Spend 和 Marketing Spend 是否共線性\n    - 共線性：兩個變數之間有很高的相關性，會導致模型不穩定，因此需要刪除其中一個變數。\n    - 透過 VIF (Variance Inflation Factor) 來檢測共線性\n        - VIF < 5 → 通常沒問題，可以放心用\n        - 5 ≤ VIF < 10 → 有點高，可能有共線性\n        - VIF ≥ 10 → 高度共線性，這個變數可能要刪掉或重新建模\n\n\n<b style=\"color: red;\">若只用來預測，VIF 值可以不用太在意，但若要解釋變數，則需要考慮 VIF 值。</b>\n\n\n2. 將 State 類別變數轉成 one-hot encoding，改成0、1編碼\n所謂 one-hot encoding，就是將類別變數轉成0、1編碼，例如：\n- State 有 3 個類別：California, New York, Florida\n- 轉成 one-hot encoding 後，變成 3 個變數：California, New York, Florida\n- 如果 State 是 California，則 California 為 1，其他為 0\n- 如果 State 是 New York，則 New York 為 1，其他為 0\n- 如果 State 是 Florida，則 Florida 為 1，其他為 0\n\n3. 建立多元線性迴歸模型\n\n::: {#5567af6a .cell execution_count=15}\n``` {.python .cell-code}\n# 計算 VIF 值\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nX = startups_df[[\"R&D Spend\", \"Marketing Spend\", \"Administration\"]]\nvif_data = pd.DataFrame()\nvif_data[\"變數\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\nprint(vif_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                變數       VIF\n0        R&D Spend  8.384571\n1  Marketing Spend  7.593984\n2   Administration  4.026406\n```\n:::\n:::\n\n\nVIF 值都小於10，保留三個變數。\n\n::: {#5ef061aa .cell execution_count=16}\n\n::: {.cell-output .cell-output-stdout}\n```\n多元線性迴歸模型: R²=0.950\n模型公式:\nProfit = 46958.945 + 0.797 * R&D Spend + 0.030 * Marketing Spend + 140.787 * State_Florida + -19.523 * State_New York\n```\n:::\n:::\n\n\n多元迴歸模型，$R^2$ 值為 0.95，比簡單線性迴歸模型(R&D Spend 解釋 Profit) 0.947 還高一些，\n但是多元迴歸模型較複雜，不好解釋，因此這裡簡單線性迴歸模型較好。\n\n#### 補充 : 簡單線性迴歸的 Gradient Descent 演算法\n\n::: {#48f2164f .cell execution_count=17}\n``` {.python .cell-code}\n# Gradient Descent 演算法\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# 建立模型五 : 簡單線性迴歸模型(Gradient Descent 演算法)\nmodel_5 = make_pipeline(\n    StandardScaler(), SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n)\n\nmodel_5.fit(startups_df[[\"R&D Spend\"]], startups_df[\"Profit\"])\nreg = model_5.named_steps[\"sgdregressor\"]  # 取出最後一層回歸器\n\n# 模型公式\nscaler = model_5.named_steps[\"standardscaler\"]\ncoef = reg.coef_[0] / scaler.scale_[0]\nintercept = reg.intercept_[0] - coef * scaler.mean_[0]\n\nprint(f\"模型公式: Profit = {intercept:.3f} + {coef:.3f} * R&D Spend\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n模型公式: Profit = 49031.729 + 0.854 * R&D Spend\n```\n:::\n:::\n\n\n用 Gradient Descent 演算法，可以得到與簡單線性迴歸模型相似的結果。\n此算法是用在資料量很大時，用來加速計算，或是用來解決非線性問題時的解法。\n\n\n### 結論與建議\n\n此範例中，先用資料概述了解資料的基本資訊，資料有50筆，有5個變數，並且無缺失值。\n另外 State 欄位是類別變數，有3個區域，是New York、California、Florida，分別有17、17、16筆資料。\n\n接著用探索分析初步了解 Profit 利潤 的分布，以及用散點圖觀察三個變數，R&D Spend 研發支出、Marketing Spend 行銷支出、Administration 管理支出，與 Profit 利潤 的關係，並且用顏色區分 State 區域。\n顏色沒有明顯的區分，但可以觀察到 研發支出 與 利潤 的關係非常強。\n另外，用相關係數熱力圖，可以看出 研發支出 與 利潤 的相關係數為 0.97，表示兩者之間有很強的正相關。\n而且 研發支出 與 行銷支出 的相關係數為 0.72，表示兩者之間也有正相關，可能需要考慮其共線性。\n\n接著建立簡單線性迴歸模型，並且計算 $R^2$ 值，可以看出 研發支出 與 利潤 的 $R^2$ 值為 0.947，表示模型能解釋 94.7% 的利潤變化。\n\n接著建立多元線性迴歸模型，並且計算 $R^2$ 值，可以看出 研發支出、行銷支出、State 的 $R^2$ 值為 0.95，表示模型能解釋 95% 的利潤變化。\n\n但是兩者的 $R^2$ 值相差不大，而且多元線性迴歸模型較複雜，不好解釋，因此這裡簡單線性迴歸模型較好。\n所以，建議用簡單線性迴歸模型，用 研發支出 來解釋 利潤 的變化。\n\n<b style=\"color: red;\">這裡重點在於迴歸模型的建立，並且只考慮模型的解釋力。若要評估模型的實用性，可以拆分訓練集與測試集，計算模型預測力。</b>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}