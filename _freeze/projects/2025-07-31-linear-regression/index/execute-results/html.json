{
  "hash": "ad809a7a1e63267432b216fc56db1c24",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"迴歸分析(Linear Regression)實例\"\ndescription: \"\"\ndate: \"2025-07-31\"\njupyter: python3\nexecute:\n    echo: false  # 是否顯示代碼\nformat:\n  html:\n    code-fold: true\n    code-summary: \"顯示／隱藏程式碼\"\n    code-tools: true\n---\n\n\n\n## 範例一：線性迴歸概念及scikit-learn 實作\n\n以動物體重與奔跑速度作為範例，使用 比較公式計算與scikit-learn 實作的結果\n\n::: {.columns}\n\n::: {.column style=\"width:45%; padding-right: 2rem;\"}\n\n### 資料\n\n::: {#55279747 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=13}\n| 動物   |   動物體重 (kg) |   最大奔跑速度 (km/h) |\n|:-------|----------------:|----------------------:|\n| 犀牛   |            1400 |                    45 |\n| 馬     |             400 |                    70 |\n| 羚羊   |              50 |                   100 |\n| 長頸鹿 |            1000 |                    60 |\n| 斑馬   |             300 |                    90 |\n| 獵豹   |              60 |                   110 |\n:::\n:::\n\n\n:::\n\n::: {.column style=\"width:55%; margin-left: auto; margin-right: 0;\"}\n\n### 散點圖\n\n::: {#fe5c601c .cell fig-format='svg' fig-height='3' fig-width='4' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=386 height=310 fig-align='center'}\n:::\n:::\n\n\n:::\n\n:::\n\n### 公式計算\n\n$$\n\\begin{align}\n\\hat{{\\mathbf{\\beta}}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\\end{align}\n$$\n\n::: {#28a7cb3a .cell execution_count=4}\n``` {.python .cell-code}\n# 依照公式 \\hat{beta} = (X^T X)^{-1} X^T y 計算參數\nX_raw = df_lm[\"動物體重 (kg)\"].to_numpy(dtype=float)\ny_vec = df_lm[\"最大奔跑速度 (km/h)\"].to_numpy(dtype=float)\n\n# 設計矩陣：第一欄為截距(常數1)，第二欄為特徵 X\nX_design = np.column_stack([np.ones(len(X_raw)), X_raw])\n\n# 依公式計算參數向量 [a, b]\nbeta_hat = np.linalg.inv(X_design.T @ X_design) @ (X_design.T @ y_vec)\na, b = float(beta_hat[0]), float(beta_hat[1])\n\nprint(f\"公式計算得出的迴歸模型: y={b:.3f}x+{a:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n公式計算得出的迴歸模型: y=-0.043x+102.200\n```\n:::\n:::\n\n\n### scikit-learn 建立迴歸模型\n\n::: {#14722fd2 .cell execution_count=5}\n``` {.python .cell-code}\n# 建立迴歸模型\nfrom sklearn.linear_model import LinearRegression\n\n# 特徵與目標（注意特徵需為 2D）\nfeatures_X = df_lm[[\"動物體重 (kg)\"]].values\ntarget_y = df_lm[\"最大奔跑速度 (km/h)\"].values\n\nmodel = LinearRegression()\n\n# 訓練模型\nmodel.fit(features_X, target_y)\n\nprint(f\"scikit-learn 計算得出的迴歸模型: y={model.coef_[0].round(3)}x+{model.intercept_.round(3)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nscikit-learn 計算得出的迴歸模型: y=-0.043x+102.2\n```\n:::\n:::\n\n\n::: {#784f0920 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=595 height=458}\n:::\n:::\n\n\n### scikit-learn 預測\n\n::: {#9325b7c2 .cell execution_count=7}\n``` {.python .cell-code}\n# 預測\nx = 1000\ny = model.predict([[x]])\n\nprint(f\"預測 {x}kg 的動物最大奔跑速度為 {y[0].round(3)} km/h\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n預測 1000kg 的動物最大奔跑速度為 59.147 km/h\n```\n:::\n:::\n\n\n## 範例二：邏輯迴歸與線性迴歸的比較\n\n假設我們想預測學生是否會及格，資料如下，X軸代表每日學習時數，Y軸代表是否及格\n\n::: {.columns}\n\n::: {.column style=\"width:45%; padding-right: 2rem;\"}\n\n\n### 資料\n\n::: {#46517da0 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=19}\n| 學生   |   學習時數 |   是否及格 |\n|:-------|-----------:|-----------:|\n| A      |          1 |          0 |\n| B      |          2 |          1 |\n| C      |          3 |          0 |\n| D      |          5 |          0 |\n| E      |          5 |          1 |\n| F      |          8 |          1 |\n| G      |          8 |          1 |\n| H      |          8 |          0 |\n| I      |          9 |          1 |\n| J      |         10 |          1 |\n:::\n:::\n\n\n:::\n\n::: {.column style=\"width:55%; margin-left: auto; margin-right: 0;\"}\n\n### 散點圖\n\n::: {#d10890b4 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=367 height=310}\n:::\n:::\n\n\n:::\n\n:::\n\n上方的資料可以明顯看出，Y軸的值屬於二元變數，是離散型的。從散點圖中可以看出，學習時數與是否及格之間的關係並非線性。\n\n使用 scikit-learn 取得模型參數，並畫出 sigmoid 曲線，紅色點為原始資料，藍色點為經過 sigmoid 函數轉換後的值，藍色線為根據原始資料所作的線性迴歸，紅色線為 sigmoid 曲線。\n\n::: {#fd913434 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\nX = logistic_df[[\"學習時數\"]].values  # 特徵（輸入）\nY = logistic_df[\"是否及格\"].values  # 標籤（目標）\n\n\n# 2. 建立並訓練邏輯回歸模型\nmodel = LogisticRegression()\nmodel.fit(X, Y)\n\n# 3. 取得模型參數（截距和斜率）\nbeta_0 = model.intercept_[0]\nbeta_1 = model.coef_[0][0]\n\nprint(f\"σ(z)模型參數：beta_0 = {beta_0:.4f}, beta_1 = {beta_1:.4f}\")\n\n\n# 4. 計算每個 X 對應的 z 和 sigmoid(z)\nz = beta_0 + beta_1 * X.flatten()\n\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\npredicted_probabilities = sigmoid(z)\n\n# 顯示每筆資料的 z 與預測機率\nfor i in range(len(X)):\n    print(f\"X = {X[i][0]}, z = {z[i]:.4f}, σ(z) = {predicted_probabilities[i]:.4f}\")\n\n# 5. 畫 sigmoid 曲線\nx_min, x_max = X.min() - 3, X.max() + 3\nx_range = np.linspace(x_min, x_max, 300)\nz_range = beta_0 + beta_1 * x_range\nsigmoid_curve = sigmoid(z_range)\n\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(7, 4))\nplt.plot(x_range, sigmoid_curve, label=\"Sigmoid Curve\", linewidth=2)\n\n# 線性迴歸\nlin_model = LinearRegression()\nlin_model.fit(X, Y)\nlin_predictions = lin_model.predict(x_range.reshape(-1, 1))\nplt.plot(x_range, lin_predictions, label=\"線性迴歸\", linestyle=\"--\", linewidth=2)\n\n\n# 資料點\nplt.scatter(X, Y, color=\"red\", label=\"Original Data (Y)\", zorder=5)\nplt.scatter(\n    X,\n    predicted_probabilities,\n    color=\"blue\",\n    label=\"Predicted Probabilities\",\n    marker=\"x\",\n    zorder=5,\n)\nplt.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\nplt.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nplt.axhline(1, color=\"gray\", linestyle=\"--\", linewidth=0.5)\nplt.xlabel(\"X（學習時數）\")\nplt.ylabel(\"Y=1 的機率\")\nplt.title(\"邏輯迴歸：Sigmoid 曲線與機率\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 1.0))\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nσ(z)模型參數：beta_0 = -1.4243, beta_1 = 0.3225\nX = 1, z = -1.1017, σ(z) = 0.2494\nX = 2, z = -0.7792, σ(z) = 0.3145\nX = 3, z = -0.4567, σ(z) = 0.3878\nX = 5, z = 0.1884, σ(z) = 0.5470\nX = 5, z = 0.1884, σ(z) = 0.5470\nX = 8, z = 1.1560, σ(z) = 0.7606\nX = 8, z = 1.1560, σ(z) = 0.7606\nX = 8, z = 1.1560, σ(z) = 0.7606\nX = 9, z = 1.4785, σ(z) = 0.8143\nX = 10, z = 1.8010, σ(z) = 0.8583\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=658 height=376}\n:::\n:::\n\n\ns型曲線更能展現出此資料分布的特性，而線性迴歸則無法。並且邏輯迴歸的預測值永遠介於 0~1，代表「Y=1 的機率」，而線性迴歸的預測值可以小於 0 或大於 1，沒有機率意義。\n\n### 預測\n\n::: {#390a28f1 .cell execution_count=11}\n``` {.python .cell-code}\n# 預測\nx_new = np.array([[0], [3],[4.5], [6], [15]])\ny_logistic = model.predict_proba(x_new)[:, 1]\ny_linear = lin_model.predict(x_new)\n\n# 建立 DataFrame 表格\ndf_pred = pd.DataFrame(\n    {\n        \"學習時數\": x_new.flatten(),\n        \"邏輯回歸預測機率 (P=1)\": y_logistic.round(3),\n        \"線性回歸預測值\": y_linear.round(3),\n    }\n)\n\n# 顯示表格\nMarkdown(df_pred.to_markdown(index=False))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=22}\n|   學習時數 |   邏輯回歸預測機率 (P=1) |   線性回歸預測值 |\n|-----------:|-------------------------:|-----------------:|\n|        0   |                    0.194 |            0.162 |\n|        3   |                    0.388 |            0.385 |\n|        4.5 |                    0.507 |            0.496 |\n|        6   |                    0.625 |            0.607 |\n|       15   |                    0.968 |            1.276 |\n:::\n:::\n\n\n- **邏輯回歸預測值**：永遠介於 0~1，代表「Y=1 的機率」\n- **線性回歸預測值**：可以小於 0 或大於 1，沒有機率意義\n\n## 範例三：線性迴歸的實際應用\n\n\n\n\n\n\n\n## 範例四:邏輯迴歸的實際應用\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}